% This is a sample document using the University of Minnesota, Morris, Computer Science
% Senior Seminar modification of the ACM sig-alternate style. Much of this content is taken
% directly from the ACM sample document illustrating the use of the sig-alternate class. Certain
% parts that we never use have been removed to simplify the example, and a few additional
% components have been added.

% See https://github.com/UMM-CSci/Senior_seminar_templates for more info and to make
% suggestions and corrections.

\documentclass{sig-alternate}
\usepackage{color}
\usepackage[colorinlistoftodos]{todonotes}

%%%%% Uncomment the following line and comment out the previous one
%%%%% to remove all comments
%%%%% NOTE: comments still occupy a line even if invisible;
%%%%% Don't write them as a separate paragraph
%\newcommand{\mycomment}[1]{}

\begin{document}

% --- Author Metadata here ---
%%% REMEMBER TO CHANGE THE SEMESTER AND YEAR AS NEEDED
\conferenceinfo{UMM CSci Senior Seminar Conference, December 2015}{Morris, MN}

\title{An Examination of Concurrent Compaction Techniques in JVM Garbage Collection}

\numberofauthors{1}

\author{
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
\alignauthor
Jacob P. Opdahl\\
	\affaddr{Division of Science and Mathematics}\\
	\affaddr{University of Minnesota, Morris}\\
	\affaddr{Morris, Minnesota, USA 56267}\\
	\email{opdah023@morris.umn.edu}
}

\maketitle
\begin{abstract}
This paper provides an overview of garbage collection (GC) of memory 
in programming languages and parallel processing as well as how 
parallel processing applies to GC; particularly, these
concepts are focused within the context of the Java Virtual Machine (JVM).
We examine in detail various algorithms that perform compaction of fragmented 
memory during garbage collection concurrently to the application running.
The desire for such GC behavior stems from a desire to reduce stop-the-world
pauses of an application.
% The current paper format *only* allows inline comments using the todo
% macro. That's kind of a bummer, and it would be neat if someone figured
% out how to change the acmconf style to allow this. I suspect it isn't *hard*
% but there are quite a few details that have to be sorted out in synchrony.
\end{abstract}

\keywords{Garbage Collection (GC), Concurrency, Compaction, Continuously Concurrent Compacting Collector (C4), Collie, Field Pinning Protocol (FPP)}


\section{Introduction}
\label{sec:introduction}

In object-oriented programming languages, the allocation and deallocation
of memory for objects can be explicit or implicit. If done implicitly,
a language is said to have \emph{automatic memory management}. Some languages 
with automatic memory management are C\#, Java, and Python.
Use of automatic memory management is beneficial to programmers as it provides
the robustness of an object-oriented language without worrying about
details unrelated to what their program is intended to do; specifically, they do
not have to worry about the details of memory allocation and deallocation. 
However, programmers also do not have control over how memory management occurs,
which can have negative impacts on application performance.

Memory for a program is not an unlimited resource. As such, automatic memory management
must ensure objects that are no longer needed are removed from memory
when appropriate. Dead objects, or \emph{garbage}, are objects that can be shown
to be unreachable by the program~\cite{glossary:g}. Thus, garbage should be deallocated to 
save space for new objects that will be created. The algorithm used to perform implicit
deallocation of garbage is referred to as a \emph{garbage collector}.
A garbage collector is a sophisticated algorithm designed to detect
and remove dead objects. We will focus on specific parts of the 
garbage collection (GC) process performed on Java Virtual Machines 
(JVMs), software processes that run Java programs on computing systems~\cite{Lindblom:2011}.

\todo[inline]{THIS IS NOT COMPLETE YET. Still need to talk briefly about why parallel GC is becoming necessary.
Might also want to say what kind of BG will be introduced and what algorithms I will look at?}


\section{Background}
\label{sec:background}

\todo[inline]{Just threw stuff from outline in this section for now.
Made a list of most bg each specific section needs so I ensure I cover
what is needed and nothing more for space. Still haven't
changed this portion since the outline}

This is where I explain terminology and aspects of GC and parallel 
computing that are necessary to understand my paper (If there is 
enough information in each subsection, I should maybe forego the 
background and just make them all sections)


\subsection{Garbage Collection}
\label{sec:garbageCollection}

References and the heap (if not done in the introduction)

General process flow GC algorithms tend to follow

Specific GC algorithms/techniques my researched papers will 
refer to and common techniques needed to understand my paper 
(Pauseless, tracing/marking, more)

Start introducing some terminology relevant to single processor 
garbage collectors such as: stop-the-world, fragmentation, compaction, 
tracing, reference remapping, reclamation etc.

Explain the two typical aspects of compaction 
(relocation/copying and remapping: 
reclamation in compacting garbage collectors is often done through these
by just labeling memory as free.)


\subsection{Parallel Processing}
\label{sec:parallelProcessing}

Threads: the virtual form of a processor (might need to 
explain virtualization or abstraction)

Issues experienced in parallel computing and reasons why it 
is difficult to perform effectively like: deadlock, race conditions, 
etc. (Real brief, but I feel the audience needs to have some idea of 
why progress in this area doesn't happen overnight. It also seems 
necessary for explaining barriers)

Typical ways of mitigating challenges experienced in parallel 
computing (synchronization, barriers/locks)

Also, talk about checkpoints here.


\subsection{Parallel Processing as it Applies to Garbage Collection}
\label{sec:parallelProcessingGarbageCollection}

Bring audience's attention back to the original focus introduced 
of wanting to use parallel computing to improve GC

Introduce new terminology for GC that depends on parallel processing 
in order to show some more specific ways in which parallel processing 
can be used to improve GC (mutator threads, parallelism, concurrency, etc.)

Explain Latency vs throughput

Describe the focus: Concurrent Compaction (If this has enough information, 
it could be moved out of being a component in a subsection)


\section{The C4 Collector}
\label{sec:c4}

%I think I got enough introductory info describing the context of C4
%here. Might want to check with Elena.
The first concurrent compaction algorithm we examine is implemented in the 
Continuously Concurrent Compacting Collector (C4), which is a garbage collector 
included in JVMs commercially shipped by Azul Systems~\cite{Tene:C4}. The 
researchers, Iyengar et al., describe how C4 is an enhanced, generational variant
of the Pauseless garbage collector~\cite{Click:Pauseless}. C4 is generational in 
that the entire GC algorithm is independently utilized in both the young and old generations 
concurrently; in addition, both generations have garbage collected concurrently 
to the application running. For set condemnation, C4 uses a tracing-style algorithm.
C4 is intended to be used in server environments with multiple GB heaps and 
multiple GB/sec allocation rates.


\subsection{Loaded Value Barrier}
\label{sec:c4LVB}

C4 uses a barrier referred to as the \emph{Loaded Value Barrier} (LVB) to 
maintain concurrency throughout the GC process~\cite{Tene:C4}. The LVB 
places invariants on each object reference value as it is loaded from memory.
One of the invariants relates to the tracing portion of the GC algorithm, so
it is not discussed here. The other invariant ensures
references to an object being relocated must point 
to wherever the object can be safely accessed, be it the from or to-space.
If the invariant does not hold
when a reference is loaded, the barrier will trigger and execute code to correct
the situation. The code executed is discussed later.

In order to prevent repetitive barrier triggers from a single incorrect reference,
the LVB uses a \emph{self-healing} technique. Whenever a reference triggers LVB
due to not meeting an invariant, it will perform the rectifying code
necessary to fix the reference. Additionally, the barrier will go to the memory
location the reference was loaded from and fix the source of the reference. This
ensures not only the loaded reference being used is fixed, but the stored reference
that might be loaded again later is also fixed.
Thus, self-healing helps reduce the occurrence of the read barrier triggers.


\subsection{Concurrent Relocation}
\label{sec:c4Relocation}

The relocation aspect of compaction in C4 occurs on a per-\emph{page}
basis, where a page is a fixed-length, contiguous block of virtual memory
that is backed by a contiguous block of physical storage~\cite{wiki:page}\cite{Tene:C4}.
To quickly empty virtual pages, the most sparsely populated pages are relocated
first. The from-space in this relocation is actually from-pages, and the to-space
is to-pages. From-pages are pages with garbage
and are marked as such during the tracing phase.

To support concurrent relocation, pages being relocated are protected. 
The LVB will trigger for mutator threads encountering a reference to an object
on a protected page. If the object has been relocated already, LVB will
obtain the new location for the mutator. If the object is being relocated, 
LVB will cause the mutator to wait
for the GC thread to finish before continuing to work.
If the object has not started relocation, LVB has the mutator thread 
escalate its movement status by moving the object itself. After any of 
these situations, the LVB will also cause the mutator thread to update 
the reference to the new location and heal the source location of the reference.
Without any mutator interference, GC threads will relocate the objects but
will not remap references at this time. The barrier check and subsequent
handling a mutator thread experiences when attempting to access an object on a
protected page can be seen from the pseudocode below adapted from~\cite{Tene:C4}.
\begin{verbatim}
LVBCheck(referenceAddress, reference)
  trigger = 0;
  // Check for tracing invariant here.
  if (pageIsProtected)
    set trigger = Reloc_Trigger;
  if (trigger != 0)
    reference = LVBHandler(referenceAddress, 
                  reference, trigger);
    
LVBHandler(address, reference, trigger)
  oldReference = reference;
  // Handle tracing trigger if that was the cause.
  if (trigger = Reloc_Trigger) 
    if (objectNotYetRelocated(reference))
      relocateObjectAt(reference);
    reference = lookupNewObjectLocation(reference);
  // Self-Healing
  compareAndSwap(address, oldReference, reference);
  return reference;
\end{verbatim}
When all living objects are relocated from a page marked for relocation,
the C4 compactor will utilize a \emph{Quick Release} method. In order to quickly recycle
physical memory sources, the physical memory backing the page will be freed
as soon as the last live object has been moved to a to page. The objects have already 
been transferred to new pages, so the contents of the from-pages are no longer needed. Thus,
physical memory backing from-pages can be used immediately. The from-page's virtual 
addresses will be in use until all references have been remapped, but this 
allows for efficient recycling of physical memory. This is made possible as the C4 compactor
stores object forwarding information outside of a from-page. Thus, when a reference to
a from-page is loaded, LVB has the mutator check the forwarding table rather than the
from-page to see where an object has moved to.


\subsection{Concurrent Remapping}
\label{sec:c4Remapping}

To maintain concurrency while updating all references to the now relocated
objects, the C4 compaction algorithm uses two techniques~\cite{Tene:C4}. The first is
referred to as \emph{lazy remapping}. With this, mutator
threads continue updating references as they trigger the LVB. In order for
the remapping phase to end, all live references must be updated.
This could go on indefinitely if lazy remapping alone is performed.

To finish remapping, a traversal of live references must be performed to
ensure all are updated; this is like the traversal performed to trace which
objects are garbage. Since no physical resources are being held due
to Quick Release and lazy remapping does not disrupt mutator operations,
the remapping traversal is pushed off until another GC cycle starts. That
is, C4 will have the remapping of one GC cycle be performed while the tracing
of another cycle is beginning. This works as both processes need to traverse
the same references in memory to find all that are reachable. To
visualize how this works, examine Figure~\ref{fig:c4Cycle}. Upon remapping completion, 
all references to from-pages are now gone. Thus, the virtual resource
can be freed and reclamation of memory is now entirely complete. 

\begin{figure}
\centering
\psfig{file=c4Mapping_rough_figure.pdf,width =3in}
\caption{C4 GC cycle. Remapping and Tracing are rolled into one traversal.
(taken from \cite{Tene:C4})}
\label{fig:c4Cycle}
\end{figure}


\subsection{C4 Results Summary}
\label{sec:c4Results}

Experiments done with the implemented C4 algorithm were meant to show improvements
from using an algorithm that is simultaneously generational and concurrent~\cite{Tene:C4}.
C4 is tested against a modified, non-generational C4 algorithm. Additionally,
it is tested against two algorithms that do not perform concurrent compaction. 
All GC algorithms were tested on the same hardware; for specifications, see~\cite{Tene:C4}. 
The test exhibited a workload with
an allocation rate of about 1.2GB/sec and live sets of objects on 
the heap consistently at a size of around 2.5GB; the actual heap size was allowed 
to grow though, indicating greater amounts of garbage on the heap. The applications were run long
enough to ensure multiple full-heap GC cycles were performed and at least one
significant compaction event occurred.

The primary performance metric monitored was the worst-case response times
of servers responding to requests while experiencing the workloads
described. This gives a sense of how much garbage collection slows
an application's responses to clients. C4 maintained the smallest 
worst-case response times across the largest range of heap sizes.
The worst-case response times were usually in the range of 0.01-0.1secs. 
These response times are fast enough to be considered \emph{pauseless}. 
The non-generational version of C4 could also reach pauseless thresholds,
but the range of heap sizes it could do so for was more limited. Where standard
C4 sustained pauseless operation for heap sizes of 5-35GB, the modified C4 could
only do so for heaps sizes of 15-35GB. The non-concurrently compacting
collectors had multi-second worst-case response times for all heap sizes experienced.


\section{The Collie Collector}
\label{sec:collie}

\todo[inline, color=green]{For professors, this should be modified enough
since my section draft to be worth reading again (or at least skimming for the main differences).}

We now look at the concurrent compaction technique used within the 
Collie garbage collector described by Iynegar et al.~\cite{Iyengar:Collie};
with the exception of Gehringer, they were the researchers who worked on the C4 algorithm.
As such, Collie utilizes modified versions of several techniques from C4, such as:
the tracing algorithm, the LVB, page-granularity compaction, self-healing, and quick-release behavior.
Only the LVB is modified in significant ways for the purposes of compaction in Collie.
The rest of the techniques are not re-discussed here due to performing 
similarly to their C4 counterparts. Like C4, the Collie
collector is also designed with server environments in mind. Collie is implemented and tested in 
the same production-quality Azul JVM the Pauseless garbage collector is implemented in
~\cite{Click:Pauseless}. However, it has not been made commercially available.


\subsection{Transactional Memory}
\label{sec:collieTM}

The Collie's compaction algorithm sets itself apart due to its use 
of a \emph{transactional memory} (TM) system as a concurrency control 
alternative to barriers~\cite{Iyengar:Collie}. TM systems allow
sections of code to function 
analogously to \emph{transactions} from database systems, 
which are a series of operations performed as one unit where 
they occur in an all-or-none manner~\cite{wiki:atomicity}.

For example, consider updating the inventory of a store; this may involve updating
the price, quantity, and distributor for various items. 
If a person updates information regarding an item, they would 
want all or none of the item's attributes to be updated. Having
only a subset of the attributes updated could cause confusion. A transaction
would ensure an item is unchanged if the process of updating the item's info 
is started and then aborted before completion.

TM systems allow a series
of operations modifying memory to be placed in
a \emph{transactional procedure}, much like a database transaction.
The TM system then monitors the access of concurrent threads to \emph{transactional variables},
memory modified by a transactional procedure.
Concurrent threads will operate in parallel until they try to modify
the same transactional variable. At this time, how 
the conflict should resolve can be specified, but the general behavior would be for one transaction
to be aborted and the other allowed to continue.
When a transaction is completed, its changes are \emph{committed}; any changes 
made to memory by the transaction are finalized~\cite{wiki:transactional-memory}.


\subsection{Object Transplantation}
\label{sec:collieTransplantation}

With the use of a TM system for synchronizing concurrent compaction,
new terminology is introduced for understanding the Collie 
compactor~\cite{Iyengar:Collie}. 
An object \emph{transplantation} consists of both the relocation and 
remapping of an object. A transplantation is not complete until
the object's contents have been moved and all required references have been
updated. All relocating collectors, and by extension all
compacting collectors, transplant objects. The Collie's compactor 
performs individual object transplantations within transactional procedures. 
Building on this, an object has a transplantation state consisting of the object's contents 
and the object's referrer set.

Each object in an application's memory has a \emph{referrer set}. This is the precise set 
of all references pointing to the object. It is crucial that an object's referrer set not
be modified or expanded during the object's transplantation. Doing so could result
in references to the object being created that the transactional procedure
was not aware of when starting, so they are not updated to point to 
the new object location when it is finishing. To avoid this, a referrer set can be
\emph{protected} by writing to each reference within it and preventing it from
being loaded temporarily.

There is also a notion of a \emph{stable} referrer set. An object has a stable referrer 
set if the object's referrer set meets the following criteria:
\begin{enumerate}
\item Once the overall GC process begins, no references to the object may be written to the heap
\item At the start of the object's transplantation, no threads contain references to the object in their stacks/registers
\item Once transplantation is underway, no new references may be added to the object's \emph{transplantation state} until it is complete
\end{enumerate}
This ensures the compactor is aware of all references to the object
and references to the object go unchanged.


\subsection{The Collie Protocol}
\label{sec:collieAlgorithm}


\subsubsection{Aborting Compactor}
\label{sec:collieAbortion}

The compactor implemented in the Collie GC algorithm uses barriers 
and transactions that will abort the transplantation process of an object if the
GC threads and mutator threads conflict to maintain concurrency~\cite{Iyengar:Collie}.
However, objects that have their transplantation aborted would then be stuck
in from-space, which would mean compaction is never performed. This is why
the additions of a \emph{mirrored-to-space} and \emph{zero-copy transplantation} are necessary.

% THere's definitely some repition here. Worry about later.
Mirrored-to-space is a virtual address space that corresponds to and is the same
size as from-space. Mirrored-to-space memory pages are mapped to the same physical
memory as their corresponding from-space pages. For barrier test and compaction 
termination purposes, mirrored-to-space is logically considered part of to-space.

Mirrored-to-space is necessary to facilitate non-compacting, zero-copy transplantation 
between from-space and to-space. An object's contents are 
consistent between from-space and mirrored-to-space and mirrored-to-space
is logically considered to be to-space. Thus, transplantation to "to-space"
can be done without copying the object's contents. That is, zero-copy transplantation
allows transplantation of an object from from-space to mirrored-to-space, which only
requires correcting references. The purpose of this is to ensure
an object makes it to "to-space" should its transplantation need 
to be aborted because mutators need to access it.

% Could make a cake example instead if this isn't really helpful.
As an example, say we have an object stored in memory. As discussed before, all references
to this object are actually references to its virtual page location. This virtual page
is backed up by some contiguous, physical memory equivalent in size to the page. 
Mirrored-to-space is simply another page that is assigned to the same section of physical memory. 
The only difference between mirrored-to-space and the page our object is stored on
is references to the object used by mutators point to its virtual page.
Zero-copy transplantation swaps this. All references to the object are
updated to point from the virtual page to mirrored-to-space. 
Thus, we get the name \emph{zero-copy} translation
because our object is never moved in physical memory; the object simply
swaps virtual pages for the purposes of the compactor believing all of from-space
has been compacted.


\subsubsection{Compaction Implementation}
\label{sec:collieAlgorithmImplementation}

After a tracing phase at the start of the overall GC algorithm for set condemnation, heap compaction
happens during a \emph{transplantation phase}~\cite{Iyengar:Collie}.
Live, relocatable objects are known at this time. The compaction 
algorithm uses TM semantics to complete an object's
transplantation. The operations for transplantation are stored as a
transactional procedure. Individual object transplantations will be done on relocatable objects 
maintaining stable referrer sets throughout the process.

An object is deemed \emph{non-individually transplantable} (NIT) if its transplantation
has to be aborted or if it fails to maintain a stable referrer set. 
Any object deemed as such will be moved to mirrored-to-space by zero-copy 
transplantation. An object may be declared NIT during the tracing phase before transplantation
if it violates requirement 1 for having a stable referrer set; discussion
of how this occurs is beyond the scope of this paper, see~\cite{Iyengar:Collie}.

% Talk about this barrier because it is during GC - betweentracing and transplantation.
% If others experience something similar, they should be talked about too.
At the start of the transplantation phase, a pre-compaction checkpoint occurs.
For all relocatable objects, referrer sets have been established during the tracing phase.
These referrer sets need to be stable referrer sets. As such, the objects are checked for 
references to them in threads' stacks and registers. Any objects with these will be 
marked NIT as their referrer sets fail to meet requirement 2 for being stable referrer sets.
After the checkpoint, the compactor has a set of relocatable objects with currently stable referrer sets.

The pre-compaction checkpoint also kicks off the LVB-style read barrier's first purpose. 
The barrier will trigger on the load of a reference to an object that was 
relocatable. Upon being triggered, the object will be marked NIT. This is done because
the load operation adds another reference to the object's referrer set, which violates
requirement 3 of having a stable referrer set.

With the set of objects to be transplanted, the compactor uses transactional
procedures to complete the individual object transplantations to to-space.
The copying of an object occurs outside the transaction since any
mutator access of the object will trigger the LVB-style read barrier,
rendering the object NIT. Thus, only the referrer set updating
is done in the transactional procedure. The transaction looks like:
\begin{enumerate}
\item Transaction is started
\item References are checked to make sure none point to the mirrored-to-space location as this means the object had been changed to NIT by some previous, interrupting operation
\item All references are changed to point from an object's from-space location to its new to-space location
\item Memory transaction is committed
\end{enumerate}
If a transaction fails or is interrupted, as detected by the TM system, then the object is rendered NIT.

After the transactional procedure either succeeds or fails for the 
relocatable objects with stable referrer sets, all that remains is to perform zero-copy transplantation
on all NIT objects. Just like the C4 compactor, Collie will do this lazily. This is the second purpose
of the LVB-like barrier; as mutators try to access NIT objects before their references are updated to
point to mirrored-to-space, the barrier will force them to correct the references. Also like C4, Collie
rolls the full traversal of memory to finish zero-copy transplantation of all NIT objects into the next
tracing phase.


\subsection{Collie Results Summary}
\label{sec:collieResults}

The main goal of the Collie collector is to maintain high mutator responsiveness
during compaction~\cite{Iyengar:Collie}. The Collie was tested 
against a modified variant of the Pauseless collector~\cite{Click:Pauseless}.
Accordingly, both collectors were implemented on the same JVM. For hardware
specifications used for testing, refer to~\cite{Iyengar:Collie}. The Collie
utilizes several aspects of the C4 collector, which in turn builds upon the Pauseless
collector. This provides an effective comparison to see how the main differences, 
use of transactional memory and an aborting compactor, affect responsiveness. 

To examine mutator responsiveness, \emph{minimum mutator utilization} (MMU) is measured.
MMU is the minimum percentage of mutators being utilized over a period of time~\cite{Bacon:MMU}. Ideally,
MMU would always be at 100\% so all mutators are being used. This provides an indicator
of the latency. MMUs were taken from various time periods during which
compaction was running and was compared between the two algorithms. Comparisons were done
with a 128MB heap and a 512MB heap. For all time windows, the Collie compactor
had higher MMUs than Pauseless. Collie did not go below a 70\% mutator utilization 
rate whereas Pauseless ranged over the whole spectrum of values. This is 
likely because the aborting nature of Collie
lends itself to ensuring mutator response time despite potentially impacting the
amount of memory compacted.


\section{Field Pinning Protocol}
\label{sec:fpp}

Last, we examine concurrent compaction in GC through the Field Pinning
Protocol (FPP) designed by \"{O}sterlund and L\"{o}we~\cite{Osterlund:FPP}.
FPP describes a lock-free process for performing the relocation aspect
of concurrent compaction. The remapping phase as well as other portions
of GC are left up to the host garbage collector this
protocol is implemented within. FPP is compliant with the Java Native Interface
to allow for integration into a JVM. For purposes of testing, the FPP 
is implemented within the Garbage-First GC algorithm in the HotSpot JVM 
of OpenJDK 9~\cite{Detlefs:G1}.


\subsection{Hazard Pointers}
\label{sec:fppHazard}

The key component driving FPP is the \emph{hazard pointer} (HP)~\cite{Osterlund:FPP}. 
In FPP, all application threads contain a list of HPs.
The HPs point to objects in memory a thread is accessing, or \emph{pin} them.
When a thread finishes accessing an object, it drops the HP.
The HPs allow for asynchronous relocation of an object. So long as an object
is pinned by a mutator, it cannot be relocated as it is still in use. Once all threads have
unpinned the object, it is ready to be relocated collaboratively.
By collaboratively, we mean the last thread to drop the HP to the object
will hold the final responsibility for copying the object.
When discussing the theoretical construction of the algorithm, a granularity of pinning 
individual fields is used, thus \emph{Field} Pinning Protocol. 

While it is explained in more detail below, an example of how HPs 
work on a high-level is useful. Imagine someone brings cake to a party. Anyone 
wanting cake must make it known to others by having a plate, otherwise the 
cake is liable to be moved to the fridge. The people are effectively pinning the cake.
Once everyone has had their share of cake, it should be moved. This can
be done safely when no one has plates any longer, so no one has the cake pinned.
The last person to discard their plate will be responsible for relocating the cake.


\subsection{Tricoloring}
\label{sec:fppColoring}

A non-garbage object enters different states of the copying process 
during the relocation phase~\cite{Osterlund:FPP}. An object is
\emph{copy-white} before and after relocation. Regardless of whether the object is
currently in to or from-space, it is copy-white when in a single location with all its
references pointing to that location. An object is \emph{copy-gray} when
being relocated or awaiting relocation. Copy-gray object's have their from-space location
point to their to-space destination and their to-space destination point to a location in
memory holding the status of the object. Last, a \emph{copy-black} object has
been relocated but still requires remapping. Copy-black objects no longer require the extra
status information. FPP moves objects marked for relocation from a copy-white state to
a copy-black state without any type of checkpoint.


\subsection{Collaborative Copying and Blame}
\label{sec:fppCopy}

We now discuss in more depth how the collaborative relocation of 
non-garbage objects occurs. When a thread impedes another thread's
attempt to copy an object, because its HP prevents copying,
the impeding thread is \emph{blamed} for the 
interruption~\cite{Osterlund:FPP}. As such, it becomes responsible for ensuring 
the object is relocated. GC threads will not interrupt
mutator threads or other GC threads because they do not actually access 
the object and pin it. Thus, a GC thread cannot receive blame;
only mutator threads can "steal" responsibility for moving an object
from another thread. 

At the start of FPP relocation, GC threads will attempt to copy
objects immediately after shading all objects marked for relocation
copy-gray. At this time, blaming is disabled. That is, mutator threads
impeding initial copy attempts will not have to attempt to move the objects
themselves. 

After the first round of attempting to copy objects, the GC threads
will again try to copy the remaining copy-gray objects. If a mutator 
thread impedes the copy attempt this time, it will be blamed. 
Once the mutator is done accessing the
object and unpins it, it will attempt to perform the copy itself. If 
any other mutators have HPs to the object at that time, the copying mutator 
will be impeded. Blame will once again be passed
along to the other mutators with HPs to the object. This sequence will
occur repeatedly until a mutator tries to copy the object while no other
mutators contain HPs to the object. 

When an object is copy-gray, it has an additional copy state associated
with it. The object has an \emph{async} state if it is in from-space
and needs to be copied. It has a \emph{sync} state if there is
ongoing relocation between from and to-space and the object needs to
be protected. The protection ensures that the copy will be impeded 
should another mutator pin the object. 
Finally, an object is \emph{copied} if it was moved
successfully during the sync state. The object can be marked
copy-black once the additional status memory is removed.

The process followed by mutator threads during the relocation phase when 
accessing an object is as follows:
\begin{enumerate}
\item Pin the object by adding a HP
\item Read the color state of the object. This tells the thread whether the object
has already been copied
\item Mutate/use the object. Since the object is pinned, the thread can 
do so without worry
\item Unpin the object when no longer needed
\item If the thread was blamed due to impeding another thread's copy attempt, 
attempt to copy the object:
\begin{enumerate}
\item Scan other threads for HPs to begin transition into sync state. If HPs found,
blame all relevant threads and move along
\item Tentatively enter sync state
\item Scan once more for HPs passing blame if found and moving along
\item Now in sync state, proceed to copy the object with further
pins still impeding the copy
\end{enumerate}
\end{enumerate}

It is possible for an
object not to move because it is continually pinned. How this terminates 
depends on the host GC algorithm used. When using the Garbage-First (G1) algorithm,
this process continues until all objects are moved or another GC cycle starts.
If objects are still copy-gray when another cycle begins, they are automatically
marked for relocation again. Allowing mutators to continue passing blame works
in G1 because, like C4 and Collie, the algorithm will roll up remapping into the next
GC cycle's tracing phase. Possible alternatives include setting a max number of impediments
or setting a timer.

% So I debated whether I should've went into as much detail as I did (sync, async, copied 
% particularly). Depending on how adviser feedback goes, I may tri too scale back the 
% technical aspects here. If I do so, I will extend the cake example to elaborate
% on how new mutators impeding move/"clean-up" works.


\subsection{FPP Results Summary}
\label{sec:fppResults}

As mentioned, FPP was implemented in the G1 garbage collector for testing purposes~\cite{Osterlund:FPP}. 
Appropriately, the modified G1 collector utilizing FPP for relocation is tested against the
unmodified G1 collector. This was tested in a largely standard MacBook Pro; for the 
hardware details, see~\cite{Osterlund:FPP}. Metrics
examined include throughput and latency.

For testing throughput, running times were captured while running benchmarks on each
JVM. Tests were run using 512MB heap sizes. Before measuring running times of the 
benchmarks, 10 warm-up iterations were run.
The added checks for HPs as well as attempting to relocate objects resulted
in roughly a 15\% loss to throughput on average. Thus, the G1 with FPP experienced consistent
performance overhead compared to the standard G1 collector.

For latency, the h2 benchmark from DaCapo is used; for details, see~\cite{Blackburn:DaCapo}.
This benchmark was chosen due to being memory intensive and using an 8GB heap;
this makes latency issues more apparent.
Additionally, latency is measured using the jHiccup tool developed by Azul Systems.
Compared to its host garbage collector, G1 with FPP improved latency at all time
intervals examined. On average, latency was improved by around 50\%. That means
pause times were about half as long. Latency was also shown to occur
mostly as a result of host garbage collector activities such as tracing and remapping.


\section{Conclusions}
\label{sec:conclusions}

To be done at a later date...

Just throwing some general final statements about each algorithm in here as I write about
them. At this point, this is not meant to be cohesive in any way. Feel free
to look at it though; let me now what you think should stick around and what is definitely
not needed.

\emph{C4}

%Not sure what else to say for this. It's a pretty solid algorithm.
As was shown in its experimentation, C4 is fully capable of sustaining pauseless application
behavior for its intended environment. C4 demonstrated two-orders of magnitude improvement
over the two non-concurrently compacting GC algorithms it was compared against. C4 is
fully concurrent, all of its main phases execute without hindering application behavior,
but the implementation of C4 tested does actually use some very brief stop-the-world synchronization
techniques between these phases. These synchronizations cause pauses in the sub-millisecond range, so
they are not significant enough for to make C4 not be considered pauseless. To be 
implemented for use on a Linux platform, C4 requires improvements to the default 
virtual mapping rates that Linux provides. In other words, default Linux platforms 
do not support high enough allocation rates to run C4 effectively. Future work 
on the C4 could include scaling it up to larger heaps and 
finding a way to remove the between phases synchronizations.

\emph{Collie}

Since the Collie uses a compactor that aborts on mutator interference to allow
concurrency, there are no copy-progress guarantees for the algorithm~\cite{Osterlund:FPP}.
While extremely unlikely, this means a GC cycle could be performed without compacting
any portion of the heap. Since the Collie reclaims memory by labeling the from-space as freed,
this would mean no garbage is actually collected should this occur.

The referrer sets built during the GC process are done per object. Keeping track of them
will require extra space overhead. The amount of overhead this would cause is discussed in
general terms, but no numbers are provided. It is noted that the overall space needed will 
depend upon the specific storage mechanisms used, but these mechanisms are not 
investigated~\cite{Iyengar:Collie}. However, allowing referrer sets to take
more space is shown to result in more memory being reclaimed for each GC cycle.

Collie is not made generational at this time, but it could be done similarly
to how C4 does so, which would improve performance. (I believe the paper
mentions some other ways Collie could be improved by borrowing from other
GC algorithms.)

\emph{FPP}

As FPP seeks to be easily implemented into a range of host garbage collectors, 
it does not require special hardware or operating system requirements. FPP
is primarily designed for tracing garbage collectors that divide the heap
into more than one region. 
FPP is mentioned to be implementable within the Pauseless garbage collector,
which we have discussed heavily influenced both the C4 and Collie garbage collectors.
It seems likely that it could also be implemented into either of them as a result.
FPP has copy-progress guarantees as well.


\section*{Acknowledgments}
\label{sec:acknowledgments}

To be done at a later date...


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
% sample_paper.bib is the name of the BibTex file containing the
% bibliography entries. Note that you *don't* include the .bib ending here.
\bibliography{sample_paper}  
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

\end{document}
